{
    "dataset_reader": {
        "type": "my_copynet_seq2seq",
        "available_languages": {
            "<DE-SRL>": 3,
            "<DE>": 2,
            "<EN-SRL>": 1,
            "<EN>": 0,
            "<FR-SRL>": 5,
            "<FR>": 4
        },
        "source_token_indexers": {
            "tokens": {
                "type": "single_id"
            }
        },
        "target_namespace": "target_tokens"
    },
    "iterator": {
        "type": "bucket",
        "batch_size": 12,
        "padding_noise": 0,
        "sorting_keys": [
            [
                "source_tokens",
                "num_tokens"
            ],
            [
                "target_tokens",
                "num_tokens"
            ]
        ]
    },
    "model": {
        "type": "seq2seq_copy_srl",
        "attention": {
            "type": "linear",
            "activation": "tanh",
            "combination": "x,y",
            "tensor_1_dim": 1024,
            "tensor_2_dim": 1024
        },
        "beam_size": 1,
        "binary_pred_feature_dim": 100,
        "encoder": {
            "type": "alternating_lstm",
            "hidden_size": 300,
            "input_size": 600,
            "num_layers": 8,
            "recurrent_dropout_probability": 0.1,
            "use_highway": true
        },
        "language_flag_dim": 200,
        "max_decoding_steps": 120,
        "number_of_languages": 6,
        "source_embedder": {
            "tokens": {
                "type": "embedding",
                "embedding_dim": 300,
                "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.300d.txt.gz",
                "trainable": true
            }
        },
        "target_embedding_dim": 500
    },
    "train_data_path": "/home/mitarb/daza/datasets/conll_srl-2009/MultilingualJSON/train/one2one_multilingual.train.json",
    "validation_data_path": "/home/mitarb/daza/datasets/conll_srl-2009/MultilingualJSON/dev/one2one_multilingual.dev.json",
    "trainer": {
        "cuda_device": 0,
        "grad_norm": 5,
        "num_epochs": 30,
        "optimizer": {
            "type": "adam",
            "lr": 0.0001
        },
        "patience": 5,
        "validation_metric": "+BLEU"
    },
    "vocabulary": {
        "max_vocab_size": {
            "source_tokens": 50000,
            "target_tokens": 30000
        }
    }
}